<a name="S1wXJ"></a>
### 设计思路
mapreduce中会合并key相同的键值对的特点，说明将内容作为key输出，则不会出现重复，因此将A、B中的内容按行分开，连续将输入的value作为map和reduce部分的key输出，就可以得到合并并不存在重复项的文件C。
<a name="xIY4l"></a>
### 程序运行结果
运行成功：<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/35539066/1698153104782-40d80c4d-5148-431a-9b6a-26db4148c94c.png#averageHue=%23300b25&clientId=u6f8ffef5-05bf-4&from=paste&height=367&id=ua1df5dfc&originHeight=643&originWidth=1510&originalType=binary&ratio=1.75&rotation=0&showTitle=false&size=233431&status=done&style=none&taskId=u2507920f-92c0-458a-b752-5c8334b89bf&title=&width=862.8571428571429)<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/35539066/1698153122851-338aed29-f910-491d-a446-1e7f2bc124e0.png#averageHue=%23300a25&clientId=u6f8ffef5-05bf-4&from=paste&height=242&id=uaef45959&originHeight=423&originWidth=1481&originalType=binary&ratio=1.75&rotation=0&showTitle=false&size=144524&status=done&style=none&taskId=u435874c2-d2ca-444b-9375-efde475fdab&title=&width=846.2857142857143)<br />输出结果：<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/35539066/1698153143199-d5b18ab4-b50a-42f0-a6fa-7710741598ab.png#averageHue=%23300a24&clientId=u6f8ffef5-05bf-4&from=paste&height=482&id=udd2ddc8d&originHeight=843&originWidth=1488&originalType=binary&ratio=1.75&rotation=0&showTitle=false&size=121939&status=done&style=none&taskId=u54d8acca-e40c-49f5-b3a6-5f3cd8d707a&title=&width=850.2857142857143)<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/35539066/1698234929634-93ca1969-99bb-485b-9672-44d8ea3888bb.png#averageHue=%23fefefd&clientId=u698cffc3-eb5d-4&from=paste&height=629&id=u1e48db95&originHeight=1100&originWidth=679&originalType=binary&ratio=1.75&rotation=0&showTitle=false&size=81337&status=done&style=none&taskId=uc25fb515-fed2-4a6f-b505-90acc9d1be4&title=&width=388)<br />web网页截图：![image](https://github.com/Vanza11/FBDP/assets/128121195/82c34626-7226-4f13-bf81-353bd29f1727)
![image.png](https://cdn.nlark.com/yuque/0/2023/png/35539066/1698155591763-98a37362-84b9-4234-a863-29133a804b0d.png#averageHue=%23fbfbfb&clientId=u6f8ffef5-05bf-4&from=paste&height=466&id=u97f83a05&originHeight=816&originWidth=1699&originalType=binary&ratio=1.75&rotation=0&showTitle=false&size=121450&status=done&style=none&taskId=u4618c11b-5e5c-4662-b271-48ffce9a8f1&title=&width=970.8571428571429)<br />提交到github个人仓库：<br />![image.png](https://cdn.nlark.com/yuque/0/2023/png/35539066/1698155437888-67a388c1-57f5-4b1a-a28b-7c575eaee611.png#averageHue=%23300a25&clientId=u6f8ffef5-05bf-4&from=paste&height=433&id=ud08dd813&originHeight=758&originWidth=1477&originalType=binary&ratio=1.75&rotation=0&showTitle=false&size=281950&status=done&style=none&taskId=u719f12c6-70bb-459f-85c7-8c24ae77b37&title=&width=844)

<a name="P6KBv"></a>
### 对性能、扩展性存在的不⾜进⾏分析

1. 性能问题：<br />数据量和内存消耗：如果输入数据量非常大，内存消耗可能会成为一个问题，特别是在Reducer端进行去重操作时。<br />改进： 可以考虑在Mapper端进行初步的去重，以减少传递到Reducer的数据量。例如，使用Bloom Filters或其他去重技术。
2. 扩展性问题：<br />集群规模：本次作业是在伪分布式部署的Haddop上运行，如果数据规模增加，可能会遇到处理速度不足等问题。<br />改进： 可以通过更换集群模式来提高处理能力，根据需求动态调整集群规模。
3. 容错性：在大规模数据处理中，节点故障可能会影响整个作业的执行。<br />改进： 使用容错技术，如数据备份，确保作业能够在部分节点故障时继续执行，提高系统的稳定性。

